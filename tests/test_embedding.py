import torch

# Ensure that we're using PyTorch in a way that matches your C++ computations
torch.set_grad_enabled(True)

Xb = torch.tensor([
    [3, 3, 6, 6, 9],
    [13, 13, 16, 16, 19],
    [23, 23, 26, 26, 1]
])

C = torch.tensor([
    [0, 0.01, 0.02, 0.03],
    [0.1, 0.11, 0.12, 0.13],
    [0.2, 0.21, 0.22, 0.23],
    [0.3, 0.31, 0.32, 0.33],
    [0.4, 0.41, 0.42, 0.43],
    [0.5, 0.51, 0.52, 0.53],
    [0.6, 0.61, 0.62, 0.63],
    [0.7, 0.71, 0.72, 0.73],
    [0.8, 0.81, 0.82, 0.83],
    [0.9, 0.91, 0.92, 0.93],
    [1.0, 1.01, 1.02, 1.03],
    [1.1, 1.11, 1.12, 1.13],
    [1.2, 1.21, 1.22, 1.23],
    [1.3, 1.31, 1.32, 1.33],
    [1.4, 1.41, 1.42, 1.43],
    [1.5, 1.51, 1.52, 1.53],
    [1.6, 1.61, 1.62, 1.63],
    [1.7, 1.71, 1.72, 1.73],
    [1.8, 1.81, 1.82, 1.83],
    [1.9, 1.91, 1.92, 1.93],
    [2.0, 2.01, 2.02, 2.03],
    [2.1, 2.11, 2.12, 2.13],
    [2.2, 2.21, 2.22, 2.23],
    [2.3, 2.31, 2.32, 2.33],
    [2.4, 2.41, 2.42, 2.43],
    [2.5, 2.51, 2.52, 2.53],
    [2.6, 2.61, 2.62, 2.63]
], requires_grad=True)

emb = C[Xb]
emb.retain_grad()

embproj = torch.reshape(emb, (3, -1))
embproj.retain_grad()

W1 = torch.tensor([
    [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0],
    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],
    [2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0],
    [3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0],
    [4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0],
    [5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0],
    [6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0],
    [7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0],
    [8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0],
    [9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0],
    [10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0],
    [11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0],
    [12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0],
    [13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0],
    [14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0],
    [15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0],
    [16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0],
    [17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0],
    [18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0],
    [19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0]
], requires_grad=True)

b1 = torch.tensor([
    -1.0,
    -2.0,
    -3.0,
    -4.0,
    -5.0,
    -6.0,
    -7.0,
    -8.0,
    -9.0,
    -10.0
], requires_grad=True)

embprojxw1 = embproj @ W1
embprojxw1.retain_grad()

out = embprojxw1 + b1
out.retain_grad()

# Backward operation
out.sum().backward()

# Print the values and gradients
to_print = [
    (Xb, "Xb"),
    (C, "C"),
    (emb, "emb"),
    (embproj, "embproj"),
    (W1, "W1"),
    (b1, "b1"),
    (embprojxw1, "embprojxw1"),
    (out, "out")
]

for tensor, name in to_print:
    print(f"{name}: ", tensor)
    print(f"{name}.grad: ", tensor.grad)
    print()